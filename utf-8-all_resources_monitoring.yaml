apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/default-container: alertmanager
    creationTimestamp: "2025-06-13T00:40:37Z"
    generateName: alertmanager-v4m-alertmanager-
    labels:
      alertmanager: v4m-alertmanager
      app.kubernetes.io/instance: v4m-alertmanager
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: alertmanager
      app.kubernetes.io/version: 0.28.0
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: alertmanager-v4m-alertmanager-5bf48ff8df
      statefulset.kubernetes.io/pod-name: alertmanager-v4m-alertmanager-0
    name: alertmanager-v4m-alertmanager-0
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: alertmanager-v4m-alertmanager
      uid: a188a9f3-7fe5-4acc-b9e1-baab5b5e7879
    resourceVersion: "70016920"
    uid: 0c993dbe-0895-46eb-a3b5-479e6dfd9067
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - alertmanager
              - key: alertmanager
                operator: In
                values:
                - v4m-alertmanager
            topologyKey: kubernetes.io/hostname
          weight: 100
    automountServiceAccountToken: true
    containers:
    - args:
      - --config.file=/etc/alertmanager/config_out/alertmanager.env.yaml
      - --storage.path=/alertmanager
      - --data.retention=240h
      - --cluster.listen-address=
      - --web.listen-address=:9093
      - --web.external-url=https://viya.sas.finances.gouv.qc.ca/alertManager
      - --web.route-prefix=/alertManager
      - --log.format=json
      - --cluster.label=monitoring/v4m-alertmanager
      - --cluster.peer=alertmanager-v4m-alertmanager-0.alertmanager-operated:9094
      - --cluster.reconnect-timeout=5m
      - --web.config.file=/etc/alertmanager/web_config/web-config.yaml
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/prometheus/alertmanager:v0.28.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /alertManager/-/healthy
          port: http-web
          scheme: HTTPS
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 3
      name: alertmanager
      ports:
      - containerPort: 9093
        name: http-web
        protocol: TCP
      - containerPort: 9094
        name: mesh-tcp
        protocol: TCP
      - containerPort: 9094
        name: mesh-udp
        protocol: UDP
      readinessProbe:
        failureThreshold: 10
        httpGet:
          path: /alertManager/-/ready
          port: http-web
          scheme: HTTPS
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        requests:
          cpu: 50m
          memory: 50Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
      - mountPath: /etc/alertmanager/config_out
        name: config-out
        readOnly: true
      - mountPath: /etc/alertmanager/certs
        name: tls-assets
        readOnly: true
      - mountPath: /alertmanager
        name: alertmanager-v4m-alertmanager-db
        subPath: alertmanager-db
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /etc/alertmanager/web_config/secret/alertmanager-tls-secret-key
        name: web-config-tls-secret-key-alertmanager-tls-secret-20cee680
        readOnly: true
      - mountPath: /etc/alertmanager/web_config/secret/alertmanager-tls-secret-cert
        name: web-config-tls-secret-cert-alertmanager-tls-secret-20cee680
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5w9fg
        readOnly: true
    - args:
      - --listen-address=:8080
      - --web-config-file=/etc/alertmanager/web_config/web-config.yaml
      - --reload-url=https://127.0.0.1:9093/alertManager/-/reload
      - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
      - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
      - --watched-dir=/etc/alertmanager/config
      - --log-format=json
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "-1"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.79.2
      imagePullPolicy: IfNotPresent
      name: config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
      - mountPath: /etc/alertmanager/config_out
        name: config-out
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /etc/alertmanager/web_config/secret/alertmanager-tls-secret-key
        name: web-config-tls-secret-key-alertmanager-tls-secret-20cee680
        readOnly: true
      - mountPath: /etc/alertmanager/web_config/secret/alertmanager-tls-secret-cert
        name: web-config-tls-secret-cert-alertmanager-tls-secret-20cee680
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5w9fg
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: alertmanager-v4m-alertmanager-0
    initContainers:
    - args:
      - --watch-interval=0
      - --listen-address=:8081
      - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
      - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
      - --watched-dir=/etc/alertmanager/config
      - --log-format=json
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "-1"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.79.2
      imagePullPolicy: IfNotPresent
      name: init-config-reloader
      ports:
      - containerPort: 8081
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
      - mountPath: /etc/alertmanager/config_out
        name: config-out
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /etc/alertmanager/web_config/secret/alertmanager-tls-secret-key
        name: web-config-tls-secret-key-alertmanager-tls-secret-20cee680
        readOnly: true
      - mountPath: /etc/alertmanager/web_config/secret/alertmanager-tls-secret-cert
        name: web-config-tls-secret-cert-alertmanager-tls-secret-20cee680
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5w9fg
        readOnly: true
    nodeName: aks-dvmsssypto01-16550377-vmss00000c
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 2000
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: v4m-alertmanager
    serviceAccountName: v4m-alertmanager
    subdomain: alertmanager-operated
    terminationGracePeriodSeconds: 120
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - name: alertmanager-v4m-alertmanager-db
      persistentVolumeClaim:
        claimName: alertmanager-v4m-alertmanager-db-alertmanager-v4m-alertmanager-0
    - name: config-volume
      secret:
        defaultMode: 420
        secretName: alertmanager-v4m-alertmanager-generated
    - name: tls-assets
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: alertmanager-v4m-alertmanager-tls-assets-0
    - emptyDir:
        medium: Memory
      name: config-out
    - name: web-config
      secret:
        defaultMode: 420
        secretName: alertmanager-v4m-alertmanager-web-config
    - name: web-config-tls-secret-key-alertmanager-tls-secret-20cee680
      secret:
        defaultMode: 420
        secretName: alertmanager-tls-secret
    - name: web-config-tls-secret-cert-alertmanager-tls-secret-20cee680
      secret:
        defaultMode: 420
        secretName: alertmanager-tls-secret
    - name: kube-api-access-5w9fg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-06-13T00:41:23Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-06-13T00:41:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-06-13T00:41:38Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-06-13T00:41:38Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-06-13T00:40:37Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://af73ed87ddfa87cda065f86ac6284530236df0db89562fa1fd96dda993d74b8d
      image: quay.io/prometheus/alertmanager:v0.28.0
      imageID: quay.io/prometheus/alertmanager@sha256:d5155cfac40a6d9250ffc97c19db2c5e190c7bc57c6b67125c94903358f8c7d8
      lastState: {}
      name: alertmanager
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-06-13T00:41:35Z"
    - containerID: containerd://707c461e876f4e2df39e6692e601a1cfd43f6bbabc8bcb627b79fae16cdc96f0
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.79.2
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:193280a33bc1acad9bc1956c5d986e3da6950882bda89311bac317998dcebf30
      lastState: {}
      name: config-reloader
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-06-13T00:41:35Z"
    hostIP: 10.73.111.134
    hostIPs:
    - ip: 10.73.111.134
    initContainerStatuses:
    - containerID: containerd://c0f8806ed23666da8be47150be03b23d9b701969c3b21b2c2968799ecc4ce125
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.79.2
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:193280a33bc1acad9bc1956c5d986e3da6950882bda89311bac317998dcebf30
      lastState: {}
      name: init-config-reloader
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://c0f8806ed23666da8be47150be03b23d9b701969c3b21b2c2968799ecc4ce125
          exitCode: 0
          finishedAt: "2025-06-13T00:41:22Z"
          reason: Completed
          startedAt: "2025-06-13T00:41:22Z"
    phase: Running
    podIP: 10.244.1.28
    podIPs:
    - ip: 10.244.1.28
    qosClass: Burstable
    startTime: "2025-06-13T00:40:37Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/default-container: prometheus
    creationTimestamp: "2025-06-13T00:40:37Z"
    generateName: prometheus-v4m-prometheus-
    labels:
      app.kubernetes.io/instance: v4m-prometheus
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/version: 3.1.0
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: prometheus-v4m-prometheus-c4884cf4
      operator.prometheus.io/name: v4m-prometheus
      operator.prometheus.io/shard: "0"
      prometheus: v4m-prometheus
      statefulset.kubernetes.io/pod-name: prometheus-v4m-prometheus-0
    name: prometheus-v4m-prometheus-0
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: prometheus-v4m-prometheus
      uid: ed59d684-5566-4f91-a8bf-b25f5b920b2c
    resourceVersion: "83113713"
    uid: d689e66f-cbd1-4eee-bdda-3e914b6c7127
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - prometheus
              - key: prometheus
                operator: In
                values:
                - v4m-prometheus
            topologyKey: kubernetes.io/hostname
          weight: 100
    automountServiceAccountToken: true
    containers:
    - args:
      - --web.console.templates=/etc/prometheus/consoles
      - --web.console.libraries=/etc/prometheus/console_libraries
      - --config.file=/etc/prometheus/config_out/prometheus.env.yaml
      - --web.enable-lifecycle
      - --web.external-url=https://viya.sas.finances.gouv.qc.ca/prometheus
      - --web.route-prefix=/prometheus
      - --log.format=json
      - --storage.tsdb.retention.time=7d
      - --storage.tsdb.retention.size=20GiB
      - --storage.tsdb.path=/prometheus
      - --storage.tsdb.wal-compression
      - --web.config.file=/etc/prometheus/web_config/web-config.yaml
      image: quay.io/prometheus/prometheus:v3.1.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 6
        httpGet:
          path: /prometheus/-/healthy
          port: http-web
          scheme: HTTPS
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      name: prometheus
      ports:
      - containerPort: 9090
        name: http-web
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /prometheus/-/ready
          port: http-web
          scheme: HTTPS
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        requests:
          cpu: "1"
          memory: 2Gi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      startupProbe:
        failureThreshold: 60
        httpGet:
          path: /prometheus/-/ready
          port: http-web
          scheme: HTTPS
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 3
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config_out
        name: config-out
        readOnly: true
      - mountPath: /etc/prometheus/certs
        name: tls-assets
        readOnly: true
      - mountPath: /prometheus
        name: prometheus-v4m-prometheus-db
        subPath: prometheus-db
      - mountPath: /etc/prometheus/rules/prometheus-v4m-prometheus-rulefiles-0
        name: prometheus-v4m-prometheus-rulefiles-0
      - mountPath: /etc/prometheus/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /etc/prometheus/web_config/secret/prometheus-tls-secret-key
        name: web-config-tls-secret-key-prometheus-tls-secret-af300a1a
        readOnly: true
      - mountPath: /etc/prometheus/web_config/secret/prometheus-tls-secret-cert
        name: web-config-tls-secret-cert-prometheus-tls-secret-af300a1a
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5cttm
        readOnly: true
    - args:
      - --listen-address=:8080
      - --web-config-file=/etc/prometheus/web_config/web-config.yaml
      - --reload-url=https://127.0.0.1:9090/prometheus/-/reload
      - --config-file=/etc/prometheus/config/prometheus.yaml.gz
      - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
      - --watched-dir=/etc/prometheus/rules/prometheus-v4m-prometheus-rulefiles-0
      - --log-format=json
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.79.2
      imagePullPolicy: IfNotPresent
      name: config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /etc/prometheus/web_config/secret/prometheus-tls-secret-key
        name: web-config-tls-secret-key-prometheus-tls-secret-af300a1a
        readOnly: true
      - mountPath: /etc/prometheus/web_config/secret/prometheus-tls-secret-cert
        name: web-config-tls-secret-cert-prometheus-tls-secret-af300a1a
        readOnly: true
      - mountPath: /etc/prometheus/rules/prometheus-v4m-prometheus-rulefiles-0
        name: prometheus-v4m-prometheus-rulefiles-0
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5cttm
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: prometheus-v4m-prometheus-0
    initContainers:
    - args:
      - --watch-interval=0
      - --listen-address=:8081
      - --config-file=/etc/prometheus/config/prometheus.yaml.gz
      - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
      - --watched-dir=/etc/prometheus/rules/prometheus-v4m-prometheus-rulefiles-0
      - --log-format=json
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.79.2
      imagePullPolicy: IfNotPresent
      name: init-config-reloader
      ports:
      - containerPort: 8081
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /etc/prometheus/web_config/secret/prometheus-tls-secret-key
        name: web-config-tls-secret-key-prometheus-tls-secret-af300a1a
        readOnly: true
      - mountPath: /etc/prometheus/web_config/secret/prometheus-tls-secret-cert
        name: web-config-tls-secret-cert-prometheus-tls-secret-af300a1a
        readOnly: true
      - mountPath: /etc/prometheus/rules/prometheus-v4m-prometheus-rulefiles-0
        name: prometheus-v4m-prometheus-rulefiles-0
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5cttm
        readOnly: true
    nodeName: aks-dvmsssypto01-16550377-vmss00000c
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 2000
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: sas-ops-acct
    serviceAccountName: sas-ops-acct
    shareProcessNamespace: false
    subdomain: prometheus-operated
    terminationGracePeriodSeconds: 600
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - name: prometheus-v4m-prometheus-db
      persistentVolumeClaim:
        claimName: prometheus-v4m-prometheus-db-prometheus-v4m-prometheus-0
    - name: config
      secret:
        defaultMode: 420
        secretName: prometheus-v4m-prometheus
    - name: tls-assets
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: prometheus-v4m-prometheus-tls-assets-0
    - emptyDir:
        medium: Memory
      name: config-out
    - configMap:
        defaultMode: 420
        name: prometheus-v4m-prometheus-rulefiles-0
      name: prometheus-v4m-prometheus-rulefiles-0
    - name: web-config
      secret:
        defaultMode: 420
        secretName: prometheus-v4m-prometheus-web-config
    - name: web-config-tls-secret-key-prometheus-tls-secret-af300a1a
      secret:
        defaultMode: 420
        secretName: prometheus-tls-secret
    - name: web-config-tls-secret-cert-prometheus-tls-secret-af300a1a
      secret:
        defaultMode: 420
        secretName: prometheus-tls-secret
    - name: kube-api-access-5cttm
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-06-13T00:41:21Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-06-13T00:41:22Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-06-27T19:05:10Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-06-27T19:05:10Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-06-13T00:40:37Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://38944b2883ed562605c55b6ec535bd1c562af338271bbb0447b34781e0909fc7
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.79.2
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:193280a33bc1acad9bc1956c5d986e3da6950882bda89311bac317998dcebf30
      lastState: {}
      name: config-reloader
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-06-13T00:41:34Z"
    - containerID: containerd://d26bc42e6d434d7cb9d6f79fbdc58e4533d3a3e2930409e9f2ed4e2669481e77
      image: quay.io/prometheus/prometheus:v3.1.0
      imageID: quay.io/prometheus/prometheus@sha256:6559acbd5d770b15bb3c954629ce190ac3cbbdb2b7f1c30f0385c4e05104e218
      lastState:
        terminated:
          containerID: containerd://05c114f4e4c597d05116093db811ac26131d4c712070cbe8b1463e1dbdb55ce6
          exitCode: 0
          finishedAt: "2025-06-27T19:04:54Z"
          reason: Completed
          startedAt: "2025-06-13T00:41:33Z"
      name: prometheus
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-06-27T19:04:54Z"
    hostIP: 10.73.111.134
    hostIPs:
    - ip: 10.73.111.134
    initContainerStatuses:
    - containerID: containerd://45f174b792b44be71c0ce43be933797ebe7add589ec25dbaf26081a954df8e88
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.79.2
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:193280a33bc1acad9bc1956c5d986e3da6950882bda89311bac317998dcebf30
      lastState: {}
      name: init-config-reloader
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://45f174b792b44be71c0ce43be933797ebe7add589ec25dbaf26081a954df8e88
          exitCode: 0
          finishedAt: "2025-06-13T00:41:20Z"
          reason: Completed
          startedAt: "2025-06-13T00:41:20Z"
    phase: Running
    podIP: 10.244.1.26
    podIPs:
    - ip: 10.244.1.26
    qosClass: Burstable
    startTime: "2025-06-13T00:40:37Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 0b4b885db2ead8a72c8e5ba9090bb2b357d97f35e065dbbd979d564ea2f8c55f
      checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
      checksum/secret: 19222196301d3567100f5d20645f776b60a0db3f44abea2a192f5bf80bbb0332
      kubectl.kubernetes.io/default-container: grafana
    creationTimestamp: "2025-06-13T00:40:36Z"
    generateName: v4m-grafana-59b95c8c48-
    labels:
      app.kubernetes.io/instance: v4m-prometheus-operator
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 11.4.0
      helm.sh/chart: grafana-8.8.4
      pod-template-hash: 59b95c8c48
    name: v4m-grafana-59b95c8c48-6rrmm
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: v4m-grafana-59b95c8c48
      uid: df85ef44-3667-4e5e-8dbb-aa7316e1ceb7
    resourceVersion: "83113600"
    uid: b79b5266-8dc2-497e-9663-0e81ea85bb34
  spec:
    automountServiceAccountToken: true
    containers:
    - env:
      - name: REQ_SKIP_TLS_VERIFY
        value: "true"
      - name: METHOD
        value: WATCH
      - name: LABEL
        value: grafana_dashboard
      - name: LABEL_VALUE
        value: "1"
      - name: FOLDER
        value: /tmp/dashboards
      - name: RESOURCE
        value: both
      - name: NAMESPACE
        value: ALL
      - name: REQ_USERNAME
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: v4m-grafana
      - name: REQ_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: v4m-grafana
      - name: REQ_URL
        value: https://localhost:3000/api/admin/provisioning/dashboards/reload
      - name: REQ_METHOD
        value: POST
      image: quay.io/kiwigrid/k8s-sidecar:1.28.0
      imagePullPolicy: IfNotPresent
      name: grafana-sc-dashboard
      resources:
        requests:
          cpu: 50m
          memory: 100Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp/dashboards
        name: sc-dashboard-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-p4ppf
        readOnly: true
    - env:
      - name: REQ_SKIP_TLS_VERIFY
        value: "true"
      - name: METHOD
        value: WATCH
      - name: LABEL
        value: grafana_datasource
      - name: LABEL_VALUE
        value: "1"
      - name: FOLDER
        value: /etc/grafana/provisioning/datasources
      - name: RESOURCE
        value: both
      - name: REQ_USERNAME
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: v4m-grafana
      - name: REQ_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: v4m-grafana
      - name: REQ_URL
        value: https://localhost:3000/api/admin/provisioning/datasources/reload
      - name: REQ_METHOD
        value: POST
      image: quay.io/kiwigrid/k8s-sidecar:1.28.0
      imagePullPolicy: IfNotPresent
      name: grafana-sc-datasources
      resources:
        requests:
          cpu: 50m
          memory: 100Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/grafana/provisioning/datasources
        name: sc-datasources-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-p4ppf
        readOnly: true
    - env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: GF_SECURITY_ADMIN_USER
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: v4m-grafana
      - name: GF_SECURITY_ADMIN_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: v4m-grafana
      - name: GF_PATHS_DATA
        value: /var/lib/grafana/
      - name: GF_PATHS_LOGS
        value: /var/log/grafana
      - name: GF_PATHS_PLUGINS
        value: /var/lib/grafana/plugins
      - name: GF_PATHS_PROVISIONING
        value: /etc/grafana/provisioning
      image: docker.io/grafana/grafana:11.4.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /api/health
          port: 3000
          scheme: HTTPS
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      name: grafana
      ports:
      - containerPort: 3000
        name: grafana
        protocol: TCP
      - containerPort: 9094
        name: gossip-tcp
        protocol: TCP
      - containerPort: 9094
        name: gossip-udp
        protocol: UDP
      - containerPort: 6060
        name: profiling
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /api/health
          port: 3000
          scheme: HTTPS
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 250m
          memory: 150Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/grafana/grafana.ini
        name: config
        subPath: grafana.ini
      - mountPath: /var/lib/grafana
        name: storage
      - mountPath: /tmp/dashboards
        name: sc-dashboard-volume
      - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
        name: sc-dashboard-provider
        subPath: provider.yaml
      - mountPath: /etc/grafana/provisioning/datasources
        name: sc-datasources-volume
      - mountPath: /cert
        name: grafana-tls
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-p4ppf
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: aks-dvmsssypto01-16550377-vmss00000c
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 472
      runAsGroup: 472
      runAsNonRoot: true
      runAsUser: 472
    serviceAccount: v4m-grafana
    serviceAccountName: v4m-grafana
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: v4m-grafana
      name: config
    - name: storage
      persistentVolumeClaim:
        claimName: v4m-grafana
    - emptyDir: {}
      name: sc-dashboard-volume
    - configMap:
        defaultMode: 420
        name: v4m-grafana-config-dashboards
      name: sc-dashboard-provider
    - emptyDir: {}
      name: sc-datasources-volume
    - name: grafana-tls
      secret:
        defaultMode: 420
        secretName: grafana-tls-secret
    - name: kube-api-access-p4ppf
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-06-13T00:41:28Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-06-13T00:40:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-06-27T19:05:00Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-06-27T19:05:00Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-06-13T00:40:36Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://515e2bc7809f4aa0d745b46807ebfb8aceed388298a6c558720a669427dd8a69
      image: docker.io/grafana/grafana:11.4.0
      imageID: docker.io/grafana/grafana@sha256:d8ea37798ccc41061a62ab080f2676dda6bf7815558499f901bdb0f533a456fb
      lastState: {}
      name: grafana
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-06-13T00:41:28Z"
    - containerID: containerd://eca64841fa852814f42032b0657cfa287e64b5c686e9b899cdbcde41665079af
      image: quay.io/kiwigrid/k8s-sidecar:1.28.0
      imageID: quay.io/kiwigrid/k8s-sidecar@sha256:4166a019eeafd1f0fef4d867dc5f224f18d84ec8681dbb31f3ca258ecf07bcf2
      lastState: {}
      name: grafana-sc-dashboard
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-06-13T00:40:53Z"
    - containerID: containerd://c373f25c97d25925665a88d312ca6b20fb88339f5bab3a7188ac859de829069d
      image: quay.io/kiwigrid/k8s-sidecar:1.28.0
      imageID: quay.io/kiwigrid/k8s-sidecar@sha256:4166a019eeafd1f0fef4d867dc5f224f18d84ec8681dbb31f3ca258ecf07bcf2
      lastState: {}
      name: grafana-sc-datasources
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-06-13T00:40:53Z"
    hostIP: 10.73.111.134
    hostIPs:
    - ip: 10.73.111.134
    phase: Running
    podIP: 10.244.1.24
    podIPs:
    - ip: 10.244.1.24
    qosClass: Burstable
    startTime: "2025-06-13T00:40:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-06-13T00:40:40Z"
    generateName: v4m-kube-state-metrics-6c6d4cb95b-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: v4m-prometheus-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.14.0
      helm.sh/chart: kube-state-metrics-5.28.0
      pod-template-hash: 6c6d4cb95b
      release: v4m-prometheus-operator
    name: v4m-kube-state-metrics-6c6d4cb95b-mw74n
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: v4m-kube-state-metrics-6c6d4cb95b
      uid: 8405b2ab-63ec-45b2-bb2b-2e5ce397383d
    resourceVersion: "70016467"
    uid: 66216761-c900-4b80-9d57-ffd9f41c9c3f
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - --metric-labels-allowlist=nodes=[*],namespaces=[*],pods=[*],deployments=[*],statefulsets=[*],daemonsets=[*],jobs=[*]
      - --port=8080
      - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
      image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.14.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /livez
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: kube-state-metrics
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        requests:
          cpu: 25m
          memory: 50Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-db8zx
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: aks-dvmsssypto01-16550377-vmss00000c
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: v4m-kube-state-metrics
    serviceAccountName: v4m-kube-state-metrics
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - name: kube-api-access-db8zx
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-06-13T00:40:52Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-06-13T00:40:40Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-06-13T00:41:00Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-06-13T00:41:00Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-06-13T00:40:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a2d6c7792af10b6c4fb1cfebd70237ce631d495c4ef9d6f0d88d9c729564c8b6
      image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.14.0
      imageID: registry.k8s.io/kube-state-metrics/kube-state-metrics@sha256:37d841299325c23b56e5951176ce8ef317d537447c0f1b2d2437dddbb1f51165
      lastState: {}
      name: kube-state-metrics
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-06-13T00:40:51Z"
    hostIP: 10.73.111.134
    hostIPs:
    - ip: 10.73.111.134
    phase: Running
    podIP: 10.244.1.23
    podIPs:
    - ip: 10.244.1.23
    qosClass: Burstable
    startTime: "2025-06-13T00:40:40Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2025-06-13T00:32:15Z"
    generateName: v4m-node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: v4m-prometheus-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.8.2
      controller-revision-hash: 646665dc5d
      helm.sh/chart: prometheus-node-exporter-4.43.1
      jobLabel: node-exporter
      pod-template-generation: "1"
      release: v4m-prometheus-operator
    name: v4m-node-exporter-54q7l
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: v4m-node-exporter
      uid: efd75b0d-e9a1-40fe-b6f9-6c76d994623e
    resourceVersion: "70008727"
    uid: cc36bb15-69e1-49ab-b0b8-05ae78e6514c
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - aks-dvmsssypto01-16550377-vmss00000c
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9110
      - --web.config.file=/opt/node-exporter/node-exporter-web.yaml
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.8.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9110
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9110
        hostPort: 9110
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9110
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
      - mountPath: /opt/node-exporter
        name: node-exporter-tls-web-config
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: aks-dvmsssypto01-16550377-vmss00000c
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: v4m-node-exporter
    serviceAccountName: v4m-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
    - configMap:
        defaultMode: 420
        name: node-exporter-tls-web-config
      name: node-exporter-tls-web-config
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-06-13T00:32:29Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-06-13T00:32:15Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-06-13T00:32:29Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-06-13T00:32:29Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-06-13T00:32:15Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://18750865e3a6357bede76ab156696f975386396b6b3b8b87cb9cf57cebc3df39
      image: quay.io/prometheus/node-exporter:v1.8.2
      imageID: quay.io/prometheus/node-exporter@sha256:4032c6d5bfd752342c3e631c2f1de93ba6b86c41db6b167b9a35372c139e7706
      lastState: {}
      name: node-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-06-13T00:32:28Z"
    hostIP: 10.73.111.134
    hostIPs:
    - ip: 10.73.111.134
    phase: Running
    podIP: 10.73.111.134
    podIPs:
    - ip: 10.73.111.134
    qosClass: BestEffort
    startTime: "2025-06-13T00:32:15Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2025-05-23T02:16:37Z"
    generateName: v4m-node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: v4m-prometheus-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.8.2
      controller-revision-hash: 646665dc5d
      helm.sh/chart: prometheus-node-exporter-4.43.1
      jobLabel: node-exporter
      pod-template-generation: "1"
      release: v4m-prometheus-operator
    name: v4m-node-exporter-8jg84
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: v4m-node-exporter
      uid: efd75b0d-e9a1-40fe-b6f9-6c76d994623e
    resourceVersion: "52043666"
    uid: 2673da46-b571-4c4a-a126-37a8baaf5605
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - aks-dvmsscspto01-16171445-vmss000009
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9110
      - --web.config.file=/opt/node-exporter/node-exporter-web.yaml
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.8.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9110
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9110
        hostPort: 9110
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9110
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
      - mountPath: /opt/node-exporter
        name: node-exporter-tls-web-config
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: aks-dvmsscspto01-16171445-vmss000009
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: v4m-node-exporter
    serviceAccountName: v4m-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
    - configMap:
        defaultMode: 420
        name: node-exporter-tls-web-config
      name: node-exporter-tls-web-config
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-05-23T02:16:50Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-05-23T02:16:37Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-05-23T02:16:50Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-05-23T02:16:50Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-05-23T02:16:37Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://318562c50830eb23f6d0aa1c67399c8556f30cd0ba2da6c4649c62f85ddcb60a
      image: quay.io/prometheus/node-exporter:v1.8.2
      imageID: quay.io/prometheus/node-exporter@sha256:4032c6d5bfd752342c3e631c2f1de93ba6b86c41db6b167b9a35372c139e7706
      lastState: {}
      name: node-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-05-23T02:16:50Z"
    hostIP: 10.73.111.138
    hostIPs:
    - ip: 10.73.111.138
    phase: Running
    podIP: 10.73.111.138
    podIPs:
    - ip: 10.73.111.138
    qosClass: BestEffort
    startTime: "2025-05-23T02:16:37Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2025-05-23T01:59:26Z"
    generateName: v4m-node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: v4m-prometheus-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.8.2
      controller-revision-hash: 646665dc5d
      helm.sh/chart: prometheus-node-exporter-4.43.1
      jobLabel: node-exporter
      pod-template-generation: "1"
      release: v4m-prometheus-operator
    name: v4m-node-exporter-cn2xf
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: v4m-node-exporter
      uid: efd75b0d-e9a1-40fe-b6f9-6c76d994623e
    resourceVersion: "52032277"
    uid: e55bdd2c-5dac-4972-b767-2cc4ef82aa7c
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - aks-dvmsscupto01-24536590-vmss000006
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9110
      - --web.config.file=/opt/node-exporter/node-exporter-web.yaml
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.8.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9110
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9110
        hostPort: 9110
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9110
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
      - mountPath: /opt/node-exporter
        name: node-exporter-tls-web-config
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: aks-dvmsscupto01-24536590-vmss000006
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: v4m-node-exporter
    serviceAccountName: v4m-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
    - configMap:
        defaultMode: 420
        name: node-exporter-tls-web-config
      name: node-exporter-tls-web-config
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-05-23T01:59:39Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-05-23T01:59:26Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-05-23T01:59:39Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-05-23T01:59:39Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-05-23T01:59:26Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://48b14db417aee81c3a668771c764458f9f192dfb77abbab7471001f15596db5d
      image: quay.io/prometheus/node-exporter:v1.8.2
      imageID: quay.io/prometheus/node-exporter@sha256:4032c6d5bfd752342c3e631c2f1de93ba6b86c41db6b167b9a35372c139e7706
      lastState: {}
      name: node-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-05-23T01:59:38Z"
    hostIP: 10.73.111.132
    hostIPs:
    - ip: 10.73.111.132
    phase: Running
    podIP: 10.73.111.132
    podIPs:
    - ip: 10.73.111.132
    qosClass: BestEffort
    startTime: "2025-05-23T01:59:26Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2025-05-23T02:13:29Z"
    generateName: v4m-node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: v4m-prometheus-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.8.2
      controller-revision-hash: 646665dc5d
      helm.sh/chart: prometheus-node-exporter-4.43.1
      jobLabel: node-exporter
      pod-template-generation: "1"
      release: v4m-prometheus-operator
    name: v4m-node-exporter-m6628
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: v4m-node-exporter
      uid: efd75b0d-e9a1-40fe-b6f9-6c76d994623e
    resourceVersion: "52041525"
    uid: bb21e6b1-c54b-41bf-a5e3-220775b29aea
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - aks-dvmsscspto01-16171445-vmss000008
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9110
      - --web.config.file=/opt/node-exporter/node-exporter-web.yaml
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.8.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9110
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9110
        hostPort: 9110
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9110
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
      - mountPath: /opt/node-exporter
        name: node-exporter-tls-web-config
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: aks-dvmsscspto01-16171445-vmss000008
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: v4m-node-exporter
    serviceAccountName: v4m-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
    - configMap:
        defaultMode: 420
        name: node-exporter-tls-web-config
      name: node-exporter-tls-web-config
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-05-23T02:13:42Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-05-23T02:13:29Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-05-23T02:13:42Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-05-23T02:13:42Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-05-23T02:13:29Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://95c1fc6b85b33bf97820c4349d932bd61c45a77174f39d489ac71d91f1fc5da8
      image: quay.io/prometheus/node-exporter:v1.8.2
      imageID: quay.io/prometheus/node-exporter@sha256:4032c6d5bfd752342c3e631c2f1de93ba6b86c41db6b167b9a35372c139e7706
      lastState: {}
      name: node-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-05-23T02:13:41Z"
    hostIP: 10.73.111.140
    hostIPs:
    - ip: 10.73.111.140
    phase: Running
    podIP: 10.73.111.140
    podIPs:
    - ip: 10.73.111.140
    qosClass: BestEffort
    startTime: "2025-05-23T02:13:29Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2025-05-23T02:10:16Z"
    generateName: v4m-node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: v4m-prometheus-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.8.2
      controller-revision-hash: 646665dc5d
      helm.sh/chart: prometheus-node-exporter-4.43.1
      jobLabel: node-exporter
      pod-template-generation: "1"
      release: v4m-prometheus-operator
    name: v4m-node-exporter-nd6cx
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: v4m-node-exporter
      uid: efd75b0d-e9a1-40fe-b6f9-6c76d994623e
    resourceVersion: "52039403"
    uid: aaaff715-158a-4948-a0a7-1db740afd50f
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - aks-dvmsscspto01-16171445-vmss000007
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9110
      - --web.config.file=/opt/node-exporter/node-exporter-web.yaml
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.8.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9110
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9110
        hostPort: 9110
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9110
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
      - mountPath: /opt/node-exporter
        name: node-exporter-tls-web-config
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: aks-dvmsscspto01-16171445-vmss000007
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: v4m-node-exporter
    serviceAccountName: v4m-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
    - configMap:
        defaultMode: 420
        name: node-exporter-tls-web-config
      name: node-exporter-tls-web-config
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-05-23T02:10:30Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-05-23T02:10:16Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-05-23T02:10:31Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-05-23T02:10:31Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-05-23T02:10:16Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://8ddc77f0b0ec1379244c7256cda2e9cb2cdfb823f9c9a94267bb10e6a33c726d
      image: quay.io/prometheus/node-exporter:v1.8.2
      imageID: quay.io/prometheus/node-exporter@sha256:4032c6d5bfd752342c3e631c2f1de93ba6b86c41db6b167b9a35372c139e7706
      lastState: {}
      name: node-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-05-23T02:10:30Z"
    hostIP: 10.73.111.137
    hostIPs:
    - ip: 10.73.111.137
    phase: Running
    podIP: 10.73.111.137
    podIPs:
    - ip: 10.73.111.137
    qosClass: BestEffort
    startTime: "2025-05-23T02:10:16Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2025-05-23T02:26:13Z"
    generateName: v4m-node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: v4m-prometheus-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.8.2
      controller-revision-hash: 646665dc5d
      helm.sh/chart: prometheus-node-exporter-4.43.1
      jobLabel: node-exporter
      pod-template-generation: "1"
      release: v4m-prometheus-operator
    name: v4m-node-exporter-rjjdh
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: v4m-node-exporter
      uid: efd75b0d-e9a1-40fe-b6f9-6c76d994623e
    resourceVersion: "86043412"
    uid: 0735bf68-e949-4d6a-9f32-b6500e7e94db
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - aks-dvmssslpto01-17576862-vmss000006
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9110
      - --web.config.file=/opt/node-exporter/node-exporter-web.yaml
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.8.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9110
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9110
        hostPort: 9110
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9110
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
      - mountPath: /opt/node-exporter
        name: node-exporter-tls-web-config
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: aks-dvmssslpto01-17576862-vmss000006
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: v4m-node-exporter
    serviceAccountName: v4m-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
    - configMap:
        defaultMode: 420
        name: node-exporter-tls-web-config
      name: node-exporter-tls-web-config
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-07-01T04:35:44Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-05-23T02:26:13Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-07-01T04:35:53Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-07-01T04:35:53Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-05-23T02:26:13Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b6ba5c3f93e9dc518bf8f577a4b7faa474dc11a12479d4dae2e981e69c9f5ef5
      image: quay.io/prometheus/node-exporter:v1.8.2
      imageID: quay.io/prometheus/node-exporter@sha256:4032c6d5bfd752342c3e631c2f1de93ba6b86c41db6b167b9a35372c139e7706
      lastState:
        terminated:
          containerID: containerd://caa2654c66cbcf1637c7e6a3e50e2da3f294166534f3e9f60538e5ae0cde5d4d
          exitCode: 255
          finishedAt: "2025-07-01T04:35:38Z"
          reason: Unknown
          startedAt: "2025-05-23T02:26:26Z"
      name: node-exporter
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-07-01T04:35:43Z"
    hostIP: 10.73.111.135
    hostIPs:
    - ip: 10.73.111.135
    phase: Running
    podIP: 10.73.111.135
    podIPs:
    - ip: 10.73.111.135
    qosClass: BestEffort
    startTime: "2025-05-23T02:26:13Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2025-05-23T02:53:20Z"
    generateName: v4m-node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: v4m-prometheus-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.8.2
      controller-revision-hash: 646665dc5d
      helm.sh/chart: prometheus-node-exporter-4.43.1
      jobLabel: node-exporter
      pod-template-generation: "1"
      release: v4m-prometheus-operator
    name: v4m-node-exporter-t9l2f
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: v4m-node-exporter
      uid: efd75b0d-e9a1-40fe-b6f9-6c76d994623e
    resourceVersion: "52075810"
    uid: 9b61ef76-a122-4cb2-b1d0-2535e1601e6b
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - aks-dvmsssfpto01-39090917-vmss000002
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9110
      - --web.config.file=/opt/node-exporter/node-exporter-web.yaml
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.8.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9110
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9110
        hostPort: 9110
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9110
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
      - mountPath: /opt/node-exporter
        name: node-exporter-tls-web-config
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: aks-dvmsssfpto01-39090917-vmss000002
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: v4m-node-exporter
    serviceAccountName: v4m-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
    - configMap:
        defaultMode: 420
        name: node-exporter-tls-web-config
      name: node-exporter-tls-web-config
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-05-23T02:53:35Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-05-23T02:53:22Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-05-23T02:53:35Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-05-23T02:53:35Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-05-23T02:53:20Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://0cad4a75d21338f274851d5059fed700788e63767df04f1bee037fb4f375d527
      image: quay.io/prometheus/node-exporter:v1.8.2
      imageID: quay.io/prometheus/node-exporter@sha256:4032c6d5bfd752342c3e631c2f1de93ba6b86c41db6b167b9a35372c139e7706
      lastState: {}
      name: node-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-05-23T02:53:34Z"
    hostIP: 10.73.111.139
    hostIPs:
    - ip: 10.73.111.139
    phase: Running
    podIP: 10.73.111.139
    podIPs:
    - ip: 10.73.111.139
    qosClass: BestEffort
    startTime: "2025-05-23T02:53:22Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2025-06-13T00:40:10Z"
    generateName: v4m-node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: v4m-prometheus-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.8.2
      controller-revision-hash: 646665dc5d
      helm.sh/chart: prometheus-node-exporter-4.43.1
      jobLabel: node-exporter
      pod-template-generation: "1"
      release: v4m-prometheus-operator
    name: v4m-node-exporter-xdn9c
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: v4m-node-exporter
      uid: efd75b0d-e9a1-40fe-b6f9-6c76d994623e
    resourceVersion: "70014965"
    uid: 83291a2f-a6b3-4381-9b4a-8ec70ed42f5b
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - aks-dvmsssypto01-16550377-vmss00000d
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9110
      - --web.config.file=/opt/node-exporter/node-exporter-web.yaml
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.8.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9110
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9110
        hostPort: 9110
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9110
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
      - mountPath: /opt/node-exporter
        name: node-exporter-tls-web-config
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: aks-dvmsssypto01-16550377-vmss00000d
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: v4m-node-exporter
    serviceAccountName: v4m-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
    - configMap:
        defaultMode: 420
        name: node-exporter-tls-web-config
      name: node-exporter-tls-web-config
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-06-13T00:40:23Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-06-13T00:40:10Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-06-13T00:40:23Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-06-13T00:40:23Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-06-13T00:40:10Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://cbfd689efdb139b7001196805fd65aa5b4039b7a60976586176c2d19ce6a69cd
      image: quay.io/prometheus/node-exporter:v1.8.2
      imageID: quay.io/prometheus/node-exporter@sha256:4032c6d5bfd752342c3e631c2f1de93ba6b86c41db6b167b9a35372c139e7706
      lastState: {}
      name: node-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-06-13T00:40:23Z"
    hostIP: 10.73.111.136
    hostIPs:
    - ip: 10.73.111.136
    phase: Running
    podIP: 10.73.111.136
    podIPs:
    - ip: 10.73.111.136
    qosClass: BestEffort
    startTime: "2025-06-13T00:40:10Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-06-13T00:40:40Z"
    generateName: v4m-operator-79cfc69f4c-
    labels:
      app: v4m-operator
      app.kubernetes.io/component: prometheus-operator
      app.kubernetes.io/instance: v4m-prometheus-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: v4m-prometheus-operator
      app.kubernetes.io/part-of: v4m
      app.kubernetes.io/version: 68.3.0
      chart: kube-prometheus-stack-68.3.0
      heritage: Helm
      pod-template-hash: 79cfc69f4c
      release: v4m-prometheus-operator
      sas.com/monitoring-base: kube-viya-monitoring
    name: v4m-operator-79cfc69f4c-5g2m5
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: v4m-operator-79cfc69f4c
      uid: 291091f5-e246-4ca7-869e-947e6bfe315a
    resourceVersion: "70016103"
    uid: d89d41db-b5ca-48da-9033-a4c945e80a01
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - --kubelet-service=kube-system/v4m-kubelet
      - --kubelet-endpoints=true
      - --kubelet-endpointslice=false
      - --log-format=json
      - --log-level=info
      - --localhost=127.0.0.1
      - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.79.2
      - --config-reloader-cpu-request=0
      - --config-reloader-cpu-limit=0
      - --config-reloader-memory-request=0
      - --config-reloader-memory-limit=0
      - --thanos-default-base-image=quay.io/thanos/thanos:v0.37.2
      - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
      - --web.enable-tls=true
      - --web.cert-file=/cert/cert
      - --web.key-file=/cert/key
      - --web.listen-address=:10250
      - --web.tls-min-version=VersionTLS13
      env:
      - name: GOGC
        value: "30"
      image: quay.io/prometheus-operator/prometheus-operator:v0.79.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: https
          scheme: HTTPS
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: v4m
      ports:
      - containerPort: 10250
        name: https
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: https
          scheme: HTTPS
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 500m
          memory: 1Gi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /cert
        name: tls-secret
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2xvl7
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: aks-dvmsssypto01-16550377-vmss00000c
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: v4m-operator
    serviceAccountName: v4m-operator
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - name: tls-secret
      secret:
        defaultMode: 420
        secretName: v4m-admission
    - name: kube-api-access-2xvl7
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-06-13T00:40:50Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-06-13T00:40:40Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-06-13T00:40:50Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-06-13T00:40:50Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-06-13T00:40:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ab8d1d2664428e7c685a0df0eb0092521f87fcb9ac709a1020c2e835a7a2ab09
      image: quay.io/prometheus-operator/prometheus-operator:v0.79.2
      imageID: quay.io/prometheus-operator/prometheus-operator@sha256:1420cefd4b20014b3361951c22593de6e9a2476bbbadd1759464eab5bfc0d34f
      lastState: {}
      name: v4m
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-06-13T00:40:49Z"
    hostIP: 10.73.111.134
    hostIPs:
    - ip: 10.73.111.134
    phase: Running
    podIP: 10.244.1.22
    podIPs:
    - ip: 10.244.1.22
    qosClass: Burstable
    startTime: "2025-06-13T00:40:40Z"
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-04-21T05:40:23Z"
    labels:
      managed-by: prometheus-operator
      operated-alertmanager: "true"
    name: alertmanager-operated
    namespace: monitoring
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      kind: Alertmanager
      name: v4m-alertmanager
      uid: 40054af7-2ced-4bc4-a193-ad3ee178c34b
    resourceVersion: "24673593"
    uid: da4f4a4d-43b2-429c-a5e6-74ba47e9c519
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9093
      protocol: TCP
      targetPort: http-web
    - name: tcp-mesh
      port: 9094
      protocol: TCP
      targetPort: 9094
    - name: udp-mesh
      port: 9094
      protocol: UDP
      targetPort: 9094
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/name: alertmanager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-04-21T05:40:23Z"
    labels:
      managed-by: prometheus-operator
      operated-prometheus: "true"
    name: prometheus-operated
    namespace: monitoring
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      kind: Prometheus
      name: v4m-prometheus
      uid: 2e442ec0-7327-47dd-b56a-01dd3f29d393
    resourceVersion: "24673611"
    uid: d05ebb9d-6d83-4498-af5b-c8e91955ea5a
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9090
      protocol: TCP
      targetPort: http-web
    selector:
      app.kubernetes.io/name: prometheus
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: v4m-prometheus-operator
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-04-21T05:40:22Z"
    labels:
      app: v4m-alertmanager
      app.kubernetes.io/instance: v4m-prometheus-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: v4m
      app.kubernetes.io/version: 68.3.0
      chart: kube-prometheus-stack-68.3.0
      heritage: Helm
      release: v4m-prometheus-operator
      sas.com/monitoring-base: kube-viya-monitoring
      self-monitor: "true"
    name: v4m-alertmanager
    namespace: monitoring
    resourceVersion: "24673388"
    uid: 2126d08a-5c81-4160-b98f-b0391ca610a4
  spec:
    clusterIP: 10.0.148.194
    clusterIPs:
    - 10.0.148.194
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9093
      protocol: TCP
      targetPort: 9093
    - appProtocol: http
      name: reloader-web
      port: 8080
      protocol: TCP
      targetPort: reloader-web
    selector:
      alertmanager: v4m-alertmanager
      app.kubernetes.io/name: alertmanager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: v4m-prometheus-operator
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-04-21T05:40:22Z"
    labels:
      app.kubernetes.io/instance: v4m-prometheus-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 11.4.0
      helm.sh/chart: grafana-8.8.4
    name: v4m-grafana
    namespace: monitoring
    resourceVersion: "24673382"
    uid: 2859acce-293a-4804-8ed4-8ade194cce18
  spec:
    clusterIP: 10.0.243.181
    clusterIPs:
    - 10.0.243.181
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 3000
      protocol: TCP
      targetPort: 3000
    selector:
      app.kubernetes.io/instance: v4m-prometheus-operator
      app.kubernetes.io/name: grafana
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: v4m-prometheus-operator
      meta.helm.sh/release-namespace: monitoring
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-04-21T05:40:22Z"
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: v4m-prometheus-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.14.0
      helm.sh/chart: kube-state-metrics-5.28.0
      release: v4m-prometheus-operator
    name: v4m-kube-state-metrics
    namespace: monitoring
    resourceVersion: "24673395"
    uid: 97c60673-5fb0-4080-88ee-85850e559165
  spec:
    clusterIP: 10.0.115.103
    clusterIPs:
    - 10.0.115.103
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app.kubernetes.io/instance: v4m-prometheus-operator
      app.kubernetes.io/name: kube-state-metrics
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: v4m-prometheus-operator
      meta.helm.sh/release-namespace: monitoring
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-04-21T05:40:22Z"
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: v4m-prometheus-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.8.2
      helm.sh/chart: prometheus-node-exporter-4.43.1
      jobLabel: node-exporter
      release: v4m-prometheus-operator
    name: v4m-node-exporter
    namespace: monitoring
    resourceVersion: "24673405"
    uid: 509b83e1-fee8-4fb6-a583-80b5a733062d
  spec:
    clusterIP: 10.0.160.100
    clusterIPs:
    - 10.0.160.100
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 9110
      protocol: TCP
      targetPort: 9110
    selector:
      app.kubernetes.io/instance: v4m-prometheus-operator
      app.kubernetes.io/name: prometheus-node-exporter
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: v4m-prometheus-operator
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-04-21T05:40:22Z"
    labels:
      app: v4m-operator
      app.kubernetes.io/component: prometheus-operator
      app.kubernetes.io/instance: v4m-prometheus-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: v4m-prometheus-operator
      app.kubernetes.io/part-of: v4m
      app.kubernetes.io/version: 68.3.0
      chart: kube-prometheus-stack-68.3.0
      heritage: Helm
      release: v4m-prometheus-operator
      sas.com/monitoring-base: kube-viya-monitoring
    name: v4m-operator
    namespace: monitoring
    resourceVersion: "24673399"
    uid: bdcd6612-cd7d-442c-8404-89783eba6282
  spec:
    clusterIP: 10.0.255.53
    clusterIPs:
    - 10.0.255.53
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      app: v4m-operator
      release: v4m-prometheus-operator
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: v4m-prometheus-operator
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-04-21T05:40:22Z"
    labels:
      app: v4m-prometheus
      app.kubernetes.io/instance: v4m-prometheus-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: v4m
      app.kubernetes.io/version: 68.3.0
      chart: kube-prometheus-stack-68.3.0
      heritage: Helm
      release: v4m-prometheus-operator
      sas.com/monitoring-base: kube-viya-monitoring
      self-monitor: "true"
    name: v4m-prometheus
    namespace: monitoring
    resourceVersion: "24673385"
    uid: 2f1486e8-c211-4251-bd4f-c4baaabfc899
  spec:
    clusterIP: 10.0.253.204
    clusterIPs:
    - 10.0.253.204
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9090
      protocol: TCP
      targetPort: 9090
    - appProtocol: http
      name: reloader-web
      port: 8080
      protocol: TCP
      targetPort: reloader-web
    selector:
      app.kubernetes.io/name: prometheus
      operator.prometheus.io/name: v4m-prometheus
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      meta.helm.sh/release-name: v4m-prometheus-operator
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-04-21T05:40:22Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: v4m-prometheus-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.8.2
      helm.sh/chart: prometheus-node-exporter-4.43.1
      release: v4m-prometheus-operator
    name: v4m-node-exporter
    namespace: monitoring
    resourceVersion: "86055345"
    uid: efd75b0d-e9a1-40fe-b6f9-6c76d994623e
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: v4m-prometheus-operator
        app.kubernetes.io/name: prometheus-node-exporter
    template:
      metadata:
        annotations:
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: v4m-prometheus-operator
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: prometheus-node-exporter
          app.kubernetes.io/part-of: prometheus-node-exporter
          app.kubernetes.io/version: 1.8.2
          helm.sh/chart: prometheus-node-exporter-4.43.1
          jobLabel: node-exporter
          release: v4m-prometheus-operator
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: eks.amazonaws.com/compute-type
                  operator: NotIn
                  values:
                  - fargate
                - key: type
                  operator: NotIn
                  values:
                  - virtual-kubelet
        automountServiceAccountToken: false
        containers:
        - args:
          - --path.procfs=/host/proc
          - --path.sysfs=/host/sys
          - --path.rootfs=/host/root
          - --path.udev.data=/host/root/run/udev/data
          - --web.listen-address=[$(HOST_IP)]:9110
          - --web.config.file=/opt/node-exporter/node-exporter-web.yaml
          env:
          - name: HOST_IP
            value: 0.0.0.0
          image: quay.io/prometheus/node-exporter:v1.8.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 9110
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: node-exporter
          ports:
          - containerPort: 9110
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 9110
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host/proc
            name: proc
            readOnly: true
          - mountPath: /host/sys
            name: sys
            readOnly: true
          - mountPath: /host/root
            mountPropagation: HostToContainer
            name: root
            readOnly: true
          - mountPath: /opt/node-exporter
            name: node-exporter-tls-web-config
        dnsPolicy: ClusterFirst
        hostNetwork: true
        hostPID: true
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: v4m-node-exporter
        serviceAccountName: v4m-node-exporter
        terminationGracePeriodSeconds: 30
        tolerations:
        - operator: Exists
        volumes:
        - hostPath:
            path: /proc
            type: ""
          name: proc
        - hostPath:
            path: /sys
            type: ""
          name: sys
        - hostPath:
            path: /
            type: ""
          name: root
        - configMap:
            defaultMode: 420
            name: node-exporter-tls-web-config
          name: node-exporter-tls-web-config
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 8
    desiredNumberScheduled: 8
    numberAvailable: 8
    numberMisscheduled: 0
    numberReady: 8
    observedGeneration: 1
    updatedNumberScheduled: 8
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: v4m-prometheus-operator
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-04-21T05:40:22Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: v4m-prometheus-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 11.4.0
      helm.sh/chart: grafana-8.8.4
    name: v4m-grafana
    namespace: monitoring
    resourceVersion: "83113604"
    uid: a9533f96-b5fc-410e-a48d-f252a4918029
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: v4m-prometheus-operator
        app.kubernetes.io/name: grafana
    strategy:
      type: Recreate
    template:
      metadata:
        annotations:
          checksum/config: 0b4b885db2ead8a72c8e5ba9090bb2b357d97f35e065dbbd979d564ea2f8c55f
          checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
          checksum/secret: 19222196301d3567100f5d20645f776b60a0db3f44abea2a192f5bf80bbb0332
          kubectl.kubernetes.io/default-container: grafana
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: v4m-prometheus-operator
          app.kubernetes.io/name: grafana
          app.kubernetes.io/version: 11.4.0
          helm.sh/chart: grafana-8.8.4
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: REQ_SKIP_TLS_VERIFY
            value: "true"
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: ALL
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: v4m-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: v4m-grafana
          - name: REQ_URL
            value: https://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.28.0
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources:
            requests:
              cpu: 50m
              memory: 100Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: REQ_SKIP_TLS_VERIFY
            value: "true"
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: v4m-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: v4m-grafana
          - name: REQ_URL
            value: https://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.28.0
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources:
            requests:
              cpu: 50m
              memory: 100Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: v4m-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: v4m-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:11.4.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTPS
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          - containerPort: 6060
            name: profiling
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 250m
              memory: 150Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
          - mountPath: /cert
            name: grafana-tls
            readOnly: true
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: v4m-grafana
        serviceAccountName: v4m-grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: v4m-grafana
          name: config
        - name: storage
          persistentVolumeClaim:
            claimName: v4m-grafana
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: v4m-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
        - name: grafana-tls
          secret:
            defaultMode: 420
            secretName: grafana-tls-secret
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-04-21T05:40:22Z"
      lastUpdateTime: "2025-04-21T05:40:43Z"
      message: ReplicaSet "v4m-grafana-59b95c8c48" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-06-27T19:05:00Z"
      lastUpdateTime: "2025-06-27T19:05:00Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: v4m-prometheus-operator
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-04-21T05:40:22Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: v4m-prometheus-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.14.0
      helm.sh/chart: kube-state-metrics-5.28.0
      release: v4m-prometheus-operator
    name: v4m-kube-state-metrics
    namespace: monitoring
    resourceVersion: "70016470"
    uid: 8051eefb-cadf-4239-9800-781351c9ce29
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: v4m-prometheus-operator
        app.kubernetes.io/name: kube-state-metrics
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: v4m-prometheus-operator
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.14.0
          helm.sh/chart: kube-state-metrics-5.28.0
          release: v4m-prometheus-operator
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --metric-labels-allowlist=nodes=[*],namespaces=[*],pods=[*],deployments=[*],statefulsets=[*],daemonsets=[*],jobs=[*]
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.14.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            requests:
              cpu: 25m
              memory: 50Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: v4m-kube-state-metrics
        serviceAccountName: v4m-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-04-21T05:40:22Z"
      lastUpdateTime: "2025-04-21T05:40:32Z"
      message: ReplicaSet "v4m-kube-state-metrics-6c6d4cb95b" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-06-13T00:41:00Z"
      lastUpdateTime: "2025-06-13T00:41:00Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: v4m-prometheus-operator
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-04-21T05:40:22Z"
    generation: 1
    labels:
      app: v4m-operator
      app.kubernetes.io/component: prometheus-operator
      app.kubernetes.io/instance: v4m-prometheus-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: v4m-prometheus-operator
      app.kubernetes.io/part-of: v4m
      app.kubernetes.io/version: 68.3.0
      chart: kube-prometheus-stack-68.3.0
      heritage: Helm
      release: v4m-prometheus-operator
      sas.com/monitoring-base: kube-viya-monitoring
    name: v4m-operator
    namespace: monitoring
    resourceVersion: "70016107"
    uid: 7cbae7da-6cb1-4f8d-821f-e483383c48ad
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: v4m-operator
        release: v4m-prometheus-operator
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: v4m-operator
          app.kubernetes.io/component: prometheus-operator
          app.kubernetes.io/instance: v4m-prometheus-operator
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: v4m-prometheus-operator
          app.kubernetes.io/part-of: v4m
          app.kubernetes.io/version: 68.3.0
          chart: kube-prometheus-stack-68.3.0
          heritage: Helm
          release: v4m-prometheus-operator
          sas.com/monitoring-base: kube-viya-monitoring
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --kubelet-service=kube-system/v4m-kubelet
          - --kubelet-endpoints=true
          - --kubelet-endpointslice=false
          - --log-format=json
          - --log-level=info
          - --localhost=127.0.0.1
          - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.79.2
          - --config-reloader-cpu-request=0
          - --config-reloader-cpu-limit=0
          - --config-reloader-memory-request=0
          - --config-reloader-memory-limit=0
          - --thanos-default-base-image=quay.io/thanos/thanos:v0.37.2
          - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
          - --web.enable-tls=true
          - --web.cert-file=/cert/cert
          - --web.key-file=/cert/key
          - --web.listen-address=:10250
          - --web.tls-min-version=VersionTLS13
          env:
          - name: GOGC
            value: "30"
          image: quay.io/prometheus-operator/prometheus-operator:v0.79.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: v4m
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 500m
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /cert
            name: tls-secret
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: v4m-operator
        serviceAccountName: v4m-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - name: tls-secret
          secret:
            defaultMode: 420
            secretName: v4m-admission
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-04-21T05:40:22Z"
      lastUpdateTime: "2025-04-21T05:40:23Z"
      message: ReplicaSet "v4m-operator-79cfc69f4c" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-06-13T00:40:50Z"
      lastUpdateTime: "2025-06-13T00:40:50Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: v4m-prometheus-operator
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-04-21T05:40:22Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: v4m-prometheus-operator
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 11.4.0
      helm.sh/chart: grafana-8.8.4
      pod-template-hash: 59b95c8c48
    name: v4m-grafana-59b95c8c48
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: v4m-grafana
      uid: a9533f96-b5fc-410e-a48d-f252a4918029
    resourceVersion: "83113601"
    uid: df85ef44-3667-4e5e-8dbb-aa7316e1ceb7
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: v4m-prometheus-operator
        app.kubernetes.io/name: grafana
        pod-template-hash: 59b95c8c48
    template:
      metadata:
        annotations:
          checksum/config: 0b4b885db2ead8a72c8e5ba9090bb2b357d97f35e065dbbd979d564ea2f8c55f
          checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
          checksum/secret: 19222196301d3567100f5d20645f776b60a0db3f44abea2a192f5bf80bbb0332
          kubectl.kubernetes.io/default-container: grafana
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: v4m-prometheus-operator
          app.kubernetes.io/name: grafana
          app.kubernetes.io/version: 11.4.0
          helm.sh/chart: grafana-8.8.4
          pod-template-hash: 59b95c8c48
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: REQ_SKIP_TLS_VERIFY
            value: "true"
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: ALL
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: v4m-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: v4m-grafana
          - name: REQ_URL
            value: https://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.28.0
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources:
            requests:
              cpu: 50m
              memory: 100Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: REQ_SKIP_TLS_VERIFY
            value: "true"
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: v4m-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: v4m-grafana
          - name: REQ_URL
            value: https://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.28.0
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources:
            requests:
              cpu: 50m
              memory: 100Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: v4m-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: v4m-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:11.4.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTPS
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          - containerPort: 6060
            name: profiling
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 250m
              memory: 150Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
          - mountPath: /cert
            name: grafana-tls
            readOnly: true
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: v4m-grafana
        serviceAccountName: v4m-grafana
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: v4m-grafana
          name: config
        - name: storage
          persistentVolumeClaim:
            claimName: v4m-grafana
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: v4m-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
        - name: grafana-tls
          secret:
            defaultMode: 420
            secretName: grafana-tls-secret
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: v4m-prometheus-operator
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-04-21T05:40:22Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: v4m-prometheus-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.14.0
      helm.sh/chart: kube-state-metrics-5.28.0
      pod-template-hash: 6c6d4cb95b
      release: v4m-prometheus-operator
    name: v4m-kube-state-metrics-6c6d4cb95b
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: v4m-kube-state-metrics
      uid: 8051eefb-cadf-4239-9800-781351c9ce29
    resourceVersion: "70016468"
    uid: 8405b2ab-63ec-45b2-bb2b-2e5ce397383d
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: v4m-prometheus-operator
        app.kubernetes.io/name: kube-state-metrics
        pod-template-hash: 6c6d4cb95b
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: v4m-prometheus-operator
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.14.0
          helm.sh/chart: kube-state-metrics-5.28.0
          pod-template-hash: 6c6d4cb95b
          release: v4m-prometheus-operator
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --metric-labels-allowlist=nodes=[*],namespaces=[*],pods=[*],deployments=[*],statefulsets=[*],daemonsets=[*],jobs=[*]
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.14.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            requests:
              cpu: 25m
              memory: 50Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: v4m-kube-state-metrics
        serviceAccountName: v4m-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: v4m-prometheus-operator
      meta.helm.sh/release-namespace: monitoring
    creationTimestamp: "2025-04-21T05:40:22Z"
    generation: 1
    labels:
      app: v4m-operator
      app.kubernetes.io/component: prometheus-operator
      app.kubernetes.io/instance: v4m-prometheus-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: v4m-prometheus-operator
      app.kubernetes.io/part-of: v4m
      app.kubernetes.io/version: 68.3.0
      chart: kube-prometheus-stack-68.3.0
      heritage: Helm
      pod-template-hash: 79cfc69f4c
      release: v4m-prometheus-operator
      sas.com/monitoring-base: kube-viya-monitoring
    name: v4m-operator-79cfc69f4c
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: v4m-operator
      uid: 7cbae7da-6cb1-4f8d-821f-e483383c48ad
    resourceVersion: "70016104"
    uid: 291091f5-e246-4ca7-869e-947e6bfe315a
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: v4m-operator
        pod-template-hash: 79cfc69f4c
        release: v4m-prometheus-operator
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: v4m-operator
          app.kubernetes.io/component: prometheus-operator
          app.kubernetes.io/instance: v4m-prometheus-operator
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: v4m-prometheus-operator
          app.kubernetes.io/part-of: v4m
          app.kubernetes.io/version: 68.3.0
          chart: kube-prometheus-stack-68.3.0
          heritage: Helm
          pod-template-hash: 79cfc69f4c
          release: v4m-prometheus-operator
          sas.com/monitoring-base: kube-viya-monitoring
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --kubelet-service=kube-system/v4m-kubelet
          - --kubelet-endpoints=true
          - --kubelet-endpointslice=false
          - --log-format=json
          - --log-level=info
          - --localhost=127.0.0.1
          - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.79.2
          - --config-reloader-cpu-request=0
          - --config-reloader-cpu-limit=0
          - --config-reloader-memory-request=0
          - --config-reloader-memory-limit=0
          - --thanos-default-base-image=quay.io/thanos/thanos:v0.37.2
          - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
          - --web.enable-tls=true
          - --web.cert-file=/cert/cert
          - --web.key-file=/cert/key
          - --web.listen-address=:10250
          - --web.tls-min-version=VersionTLS13
          env:
          - name: GOGC
            value: "30"
          image: quay.io/prometheus-operator/prometheus-operator:v0.79.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: v4m
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 500m
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /cert
            name: tls-secret
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: v4m-operator
        serviceAccountName: v4m-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - name: tls-secret
          secret:
            defaultMode: 420
            secretName: v4m-admission
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: v4m-prometheus-operator
      meta.helm.sh/release-namespace: monitoring
      prometheus-operator-input-hash: "11380133107860262070"
    creationTimestamp: "2025-04-21T05:40:23Z"
    generation: 1
    labels:
      app: v4m-alertmanager
      app.kubernetes.io/instance: v4m-prometheus-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: v4m
      app.kubernetes.io/version: 68.3.0
      chart: kube-prometheus-stack-68.3.0
      heritage: Helm
      managed-by: prometheus-operator
      release: v4m-prometheus-operator
      sas.com/monitoring-base: kube-viya-monitoring
    name: alertmanager-v4m-alertmanager
    namespace: monitoring
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      blockOwnerDeletion: true
      controller: true
      kind: Alertmanager
      name: v4m-alertmanager
      uid: 40054af7-2ced-4bc4-a193-ad3ee178c34b
    resourceVersion: "70016922"
    uid: a188a9f3-7fe5-4acc-b9e1-baab5b5e7879
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: Parallel
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        alertmanager: v4m-alertmanager
        app.kubernetes.io/instance: v4m-alertmanager
        app.kubernetes.io/managed-by: prometheus-operator
        app.kubernetes.io/name: alertmanager
    serviceName: alertmanager-operated
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/default-container: alertmanager
        creationTimestamp: null
        labels:
          alertmanager: v4m-alertmanager
          app.kubernetes.io/instance: v4m-alertmanager
          app.kubernetes.io/managed-by: prometheus-operator
          app.kubernetes.io/name: alertmanager
          app.kubernetes.io/version: 0.28.0
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - alertmanager
                  - key: alertmanager
                    operator: In
                    values:
                    - v4m-alertmanager
                topologyKey: kubernetes.io/hostname
              weight: 100
        automountServiceAccountToken: true
        containers:
        - args:
          - --config.file=/etc/alertmanager/config_out/alertmanager.env.yaml
          - --storage.path=/alertmanager
          - --data.retention=240h
          - --cluster.listen-address=
          - --web.listen-address=:9093
          - --web.external-url=https://viya.sas.finances.gouv.qc.ca/alertManager
          - --web.route-prefix=/alertManager
          - --log.format=json
          - --cluster.label=monitoring/v4m-alertmanager
          - --cluster.peer=alertmanager-v4m-alertmanager-0.alertmanager-operated:9094
          - --cluster.reconnect-timeout=5m
          - --web.config.file=/etc/alertmanager/web_config/web-config.yaml
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/prometheus/alertmanager:v0.28.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /alertManager/-/healthy
              port: http-web
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
          name: alertmanager
          ports:
          - containerPort: 9093
            name: http-web
            protocol: TCP
          - containerPort: 9094
            name: mesh-tcp
            protocol: TCP
          - containerPort: 9094
            name: mesh-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 10
            httpGet:
              path: /alertManager/-/ready
              port: http-web
              scheme: HTTPS
            initialDelaySeconds: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            requests:
              cpu: 50m
              memory: 50Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/alertmanager/config
            name: config-volume
          - mountPath: /etc/alertmanager/config_out
            name: config-out
            readOnly: true
          - mountPath: /etc/alertmanager/certs
            name: tls-assets
            readOnly: true
          - mountPath: /alertmanager
            name: alertmanager-v4m-alertmanager-db
            subPath: alertmanager-db
          - mountPath: /etc/alertmanager/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
          - mountPath: /etc/alertmanager/web_config/secret/alertmanager-tls-secret-key
            name: web-config-tls-secret-key-alertmanager-tls-secret-20cee680
            readOnly: true
          - mountPath: /etc/alertmanager/web_config/secret/alertmanager-tls-secret-cert
            name: web-config-tls-secret-cert-alertmanager-tls-secret-20cee680
            readOnly: true
        - args:
          - --listen-address=:8080
          - --web-config-file=/etc/alertmanager/web_config/web-config.yaml
          - --reload-url=https://127.0.0.1:9093/alertManager/-/reload
          - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
          - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
          - --watched-dir=/etc/alertmanager/config
          - --log-format=json
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "-1"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.79.2
          imagePullPolicy: IfNotPresent
          name: config-reloader
          ports:
          - containerPort: 8080
            name: reloader-web
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/alertmanager/config
            name: config-volume
            readOnly: true
          - mountPath: /etc/alertmanager/config_out
            name: config-out
          - mountPath: /etc/alertmanager/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
          - mountPath: /etc/alertmanager/web_config/secret/alertmanager-tls-secret-key
            name: web-config-tls-secret-key-alertmanager-tls-secret-20cee680
            readOnly: true
          - mountPath: /etc/alertmanager/web_config/secret/alertmanager-tls-secret-cert
            name: web-config-tls-secret-cert-alertmanager-tls-secret-20cee680
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --watch-interval=0
          - --listen-address=:8081
          - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
          - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
          - --watched-dir=/etc/alertmanager/config
          - --log-format=json
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "-1"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.79.2
          imagePullPolicy: IfNotPresent
          name: init-config-reloader
          ports:
          - containerPort: 8081
            name: reloader-web
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/alertmanager/config
            name: config-volume
            readOnly: true
          - mountPath: /etc/alertmanager/config_out
            name: config-out
          - mountPath: /etc/alertmanager/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
          - mountPath: /etc/alertmanager/web_config/secret/alertmanager-tls-secret-key
            name: web-config-tls-secret-key-alertmanager-tls-secret-20cee680
            readOnly: true
          - mountPath: /etc/alertmanager/web_config/secret/alertmanager-tls-secret-cert
            name: web-config-tls-secret-cert-alertmanager-tls-secret-20cee680
            readOnly: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 2000
          runAsGroup: 2000
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: v4m-alertmanager
        serviceAccountName: v4m-alertmanager
        terminationGracePeriodSeconds: 120
        volumes:
        - name: config-volume
          secret:
            defaultMode: 420
            secretName: alertmanager-v4m-alertmanager-generated
        - name: tls-assets
          projected:
            defaultMode: 420
            sources:
            - secret:
                name: alertmanager-v4m-alertmanager-tls-assets-0
        - emptyDir:
            medium: Memory
          name: config-out
        - name: web-config
          secret:
            defaultMode: 420
            secretName: alertmanager-v4m-alertmanager-web-config
        - name: web-config-tls-secret-key-alertmanager-tls-secret-20cee680
          secret:
            defaultMode: 420
            secretName: alertmanager-tls-secret
        - name: web-config-tls-secret-cert-alertmanager-tls-secret-20cee680
          secret:
            defaultMode: 420
            secretName: alertmanager-tls-secret
    updateStrategy:
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: alertmanager-v4m-alertmanager-db
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 10Gi
        storageClassName: sas
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: alertmanager-v4m-alertmanager-5bf48ff8df
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: alertmanager-v4m-alertmanager-5bf48ff8df
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: v4m-prometheus-operator
      meta.helm.sh/release-namespace: monitoring
      prometheus-operator-input-hash: "15405395193908962543"
    creationTimestamp: "2025-04-21T05:40:23Z"
    generation: 1
    labels:
      app: v4m-prometheus
      app.kubernetes.io/instance: v4m-prometheus-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: v4m
      app.kubernetes.io/version: 68.3.0
      chart: kube-prometheus-stack-68.3.0
      heritage: Helm
      managed-by: prometheus-operator
      operator.prometheus.io/mode: server
      operator.prometheus.io/name: v4m-prometheus
      operator.prometheus.io/shard: "0"
      release: v4m-prometheus-operator
      sas.com/monitoring-base: kube-viya-monitoring
    name: prometheus-v4m-prometheus
    namespace: monitoring
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      blockOwnerDeletion: true
      controller: true
      kind: Prometheus
      name: v4m-prometheus
      uid: 2e442ec0-7327-47dd-b56a-01dd3f29d393
    resourceVersion: "83113715"
    uid: ed59d684-5566-4f91-a8bf-b25f5b920b2c
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: Parallel
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: v4m-prometheus
        app.kubernetes.io/managed-by: prometheus-operator
        app.kubernetes.io/name: prometheus
        operator.prometheus.io/name: v4m-prometheus
        operator.prometheus.io/shard: "0"
        prometheus: v4m-prometheus
    serviceName: prometheus-operated
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/default-container: prometheus
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: v4m-prometheus
          app.kubernetes.io/managed-by: prometheus-operator
          app.kubernetes.io/name: prometheus
          app.kubernetes.io/version: 3.1.0
          operator.prometheus.io/name: v4m-prometheus
          operator.prometheus.io/shard: "0"
          prometheus: v4m-prometheus
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - prometheus
                  - key: prometheus
                    operator: In
                    values:
                    - v4m-prometheus
                topologyKey: kubernetes.io/hostname
              weight: 100
        automountServiceAccountToken: true
        containers:
        - args:
          - --web.console.templates=/etc/prometheus/consoles
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --config.file=/etc/prometheus/config_out/prometheus.env.yaml
          - --web.enable-lifecycle
          - --web.external-url=https://viya.sas.finances.gouv.qc.ca/prometheus
          - --web.route-prefix=/prometheus
          - --log.format=json
          - --storage.tsdb.retention.time=7d
          - --storage.tsdb.retention.size=20GiB
          - --storage.tsdb.path=/prometheus
          - --storage.tsdb.wal-compression
          - --web.config.file=/etc/prometheus/web_config/web-config.yaml
          image: quay.io/prometheus/prometheus:v3.1.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /prometheus/-/healthy
              port: http-web
              scheme: HTTPS
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          name: prometheus
          ports:
          - containerPort: 9090
            name: http-web
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /prometheus/-/ready
              port: http-web
              scheme: HTTPS
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /prometheus/-/ready
              port: http-web
              scheme: HTTPS
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 3
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/prometheus/config_out
            name: config-out
            readOnly: true
          - mountPath: /etc/prometheus/certs
            name: tls-assets
            readOnly: true
          - mountPath: /prometheus
            name: prometheus-v4m-prometheus-db
            subPath: prometheus-db
          - mountPath: /etc/prometheus/rules/prometheus-v4m-prometheus-rulefiles-0
            name: prometheus-v4m-prometheus-rulefiles-0
          - mountPath: /etc/prometheus/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
          - mountPath: /etc/prometheus/web_config/secret/prometheus-tls-secret-key
            name: web-config-tls-secret-key-prometheus-tls-secret-af300a1a
            readOnly: true
          - mountPath: /etc/prometheus/web_config/secret/prometheus-tls-secret-cert
            name: web-config-tls-secret-cert-prometheus-tls-secret-af300a1a
            readOnly: true
        - args:
          - --listen-address=:8080
          - --web-config-file=/etc/prometheus/web_config/web-config.yaml
          - --reload-url=https://127.0.0.1:9090/prometheus/-/reload
          - --config-file=/etc/prometheus/config/prometheus.yaml.gz
          - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
          - --watched-dir=/etc/prometheus/rules/prometheus-v4m-prometheus-rulefiles-0
          - --log-format=json
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "0"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.79.2
          imagePullPolicy: IfNotPresent
          name: config-reloader
          ports:
          - containerPort: 8080
            name: reloader-web
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/prometheus/config
            name: config
          - mountPath: /etc/prometheus/config_out
            name: config-out
          - mountPath: /etc/prometheus/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
          - mountPath: /etc/prometheus/web_config/secret/prometheus-tls-secret-key
            name: web-config-tls-secret-key-prometheus-tls-secret-af300a1a
            readOnly: true
          - mountPath: /etc/prometheus/web_config/secret/prometheus-tls-secret-cert
            name: web-config-tls-secret-cert-prometheus-tls-secret-af300a1a
            readOnly: true
          - mountPath: /etc/prometheus/rules/prometheus-v4m-prometheus-rulefiles-0
            name: prometheus-v4m-prometheus-rulefiles-0
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --watch-interval=0
          - --listen-address=:8081
          - --config-file=/etc/prometheus/config/prometheus.yaml.gz
          - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
          - --watched-dir=/etc/prometheus/rules/prometheus-v4m-prometheus-rulefiles-0
          - --log-format=json
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "0"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.79.2
          imagePullPolicy: IfNotPresent
          name: init-config-reloader
          ports:
          - containerPort: 8081
            name: reloader-web
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/prometheus/config
            name: config
          - mountPath: /etc/prometheus/config_out
            name: config-out
          - mountPath: /etc/prometheus/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
          - mountPath: /etc/prometheus/web_config/secret/prometheus-tls-secret-key
            name: web-config-tls-secret-key-prometheus-tls-secret-af300a1a
            readOnly: true
          - mountPath: /etc/prometheus/web_config/secret/prometheus-tls-secret-cert
            name: web-config-tls-secret-cert-prometheus-tls-secret-af300a1a
            readOnly: true
          - mountPath: /etc/prometheus/rules/prometheus-v4m-prometheus-rulefiles-0
            name: prometheus-v4m-prometheus-rulefiles-0
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 2000
          runAsGroup: 2000
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: sas-ops-acct
        serviceAccountName: sas-ops-acct
        shareProcessNamespace: false
        terminationGracePeriodSeconds: 600
        volumes:
        - name: config
          secret:
            defaultMode: 420
            secretName: prometheus-v4m-prometheus
        - name: tls-assets
          projected:
            defaultMode: 420
            sources:
            - secret:
                name: prometheus-v4m-prometheus-tls-assets-0
        - emptyDir:
            medium: Memory
          name: config-out
        - configMap:
            defaultMode: 420
            name: prometheus-v4m-prometheus-rulefiles-0
          name: prometheus-v4m-prometheus-rulefiles-0
        - name: web-config
          secret:
            defaultMode: 420
            secretName: prometheus-v4m-prometheus-web-config
        - name: web-config-tls-secret-key-prometheus-tls-secret-af300a1a
          secret:
            defaultMode: 420
            secretName: prometheus-tls-secret
        - name: web-config-tls-secret-cert-prometheus-tls-secret-af300a1a
          secret:
            defaultMode: 420
            secretName: prometheus-tls-secret
    updateStrategy:
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: prometheus-v4m-prometheus-db
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 25Gi
        storageClassName: sas
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: prometheus-v4m-prometheus-c4884cf4
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: prometheus-v4m-prometheus-c4884cf4
    updatedReplicas: 1
kind: List
metadata:
  resourceVersion: ""
